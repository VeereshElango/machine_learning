{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Automatic Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Recurrent Neural Networks in Python with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have followed the tutorial from this link http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have reduced the file size by taking only the first 500 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  26698\n",
      "Total Vocab:  41\n",
      "Total Patterns:  26598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 3.0312Epoch 00000: loss improved from inf to 3.03107, saving model to weights-improvement-00-3.0311-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 775s - loss: 3.0311   \n",
      "Epoch 2/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.8550Epoch 00001: loss improved from 3.03107 to 2.85442, saving model to weights-improvement-01-2.8544-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 803s - loss: 2.8544   \n",
      "Epoch 3/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.7148Epoch 00002: loss improved from 2.85442 to 2.71480, saving model to weights-improvement-02-2.7148-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 811s - loss: 2.7148   \n",
      "Epoch 4/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.6398Epoch 00003: loss improved from 2.71480 to 2.63966, saving model to weights-improvement-03-2.6397-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 809s - loss: 2.6397   \n",
      "Epoch 5/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.5700Epoch 00004: loss improved from 2.63966 to 2.56988, saving model to weights-improvement-04-2.5699-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 807s - loss: 2.5699   \n",
      "Epoch 6/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.5081Epoch 00005: loss improved from 2.56988 to 2.50777, saving model to weights-improvement-05-2.5078-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 803s - loss: 2.5078   \n",
      "Epoch 7/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.4419Epoch 00006: loss improved from 2.50777 to 2.44189, saving model to weights-improvement-06-2.4419-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 797s - loss: 2.4419   \n",
      "Epoch 8/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.3721Epoch 00007: loss improved from 2.44189 to 2.37249, saving model to weights-improvement-07-2.3725-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 798s - loss: 2.3725   \n",
      "Epoch 9/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.3119Epoch 00008: loss improved from 2.37249 to 2.31182, saving model to weights-improvement-08-2.3118-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 803s - loss: 2.3118   \n",
      "Epoch 10/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.2498Epoch 00009: loss improved from 2.31182 to 2.24904, saving model to weights-improvement-09-2.2490-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 803s - loss: 2.2490   \n",
      "Epoch 11/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.1881Epoch 00010: loss improved from 2.24904 to 2.18811, saving model to weights-improvement-10-2.1881-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 808s - loss: 2.1881   \n",
      "Epoch 12/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.1315Epoch 00011: loss improved from 2.18811 to 2.13210, saving model to weights-improvement-11-2.1321-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1131s - loss: 2.1321  \n",
      "Epoch 13/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.0727Epoch 00012: loss improved from 2.13210 to 2.07256, saving model to weights-improvement-12-2.0726-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1157s - loss: 2.0726  \n",
      "Epoch 14/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 2.0172Epoch 00013: loss improved from 2.07256 to 2.01759, saving model to weights-improvement-13-2.0176-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1058s - loss: 2.0176  \n",
      "Epoch 15/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.9591Epoch 00014: loss improved from 2.01759 to 1.95891, saving model to weights-improvement-14-1.9589-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1215s - loss: 1.9589  \n",
      "Epoch 16/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.9043Epoch 00015: loss improved from 1.95891 to 1.90462, saving model to weights-improvement-15-1.9046-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1276s - loss: 1.9046  \n",
      "Epoch 17/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.8401Epoch 00016: loss improved from 1.90462 to 1.84002, saving model to weights-improvement-16-1.8400-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1063s - loss: 1.8400  \n",
      "Epoch 18/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.7877Epoch 00017: loss improved from 1.84002 to 1.78797, saving model to weights-improvement-17-1.7880-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1168s - loss: 1.7880  \n",
      "Epoch 19/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.7357Epoch 00018: loss improved from 1.78797 to 1.73586, saving model to weights-improvement-18-1.7359-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 929s - loss: 1.7359   \n",
      "Epoch 20/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.6764Epoch 00019: loss improved from 1.73586 to 1.67611, saving model to weights-improvement-19-1.6761-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 946s - loss: 1.6761   \n",
      "Epoch 21/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.6255Epoch 00020: loss improved from 1.67611 to 1.62587, saving model to weights-improvement-20-1.6259-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 896s - loss: 1.6259   \n",
      "Epoch 22/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.5766Epoch 00021: loss improved from 1.62587 to 1.57678, saving model to weights-improvement-21-1.5768-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1002s - loss: 1.5768  \n",
      "Epoch 23/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.5240Epoch 00022: loss improved from 1.57678 to 1.52397, saving model to weights-improvement-22-1.5240-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1156s - loss: 1.5240  \n",
      "Epoch 24/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.4747Epoch 00023: loss improved from 1.52397 to 1.47431, saving model to weights-improvement-23-1.4743-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1074s - loss: 1.4743  \n",
      "Epoch 25/50\n",
      "26560/26598 [============================>.] - ETA: 2s - loss: 1.4343Epoch 00024: loss improved from 1.47431 to 1.43414, saving model to weights-improvement-24-1.4341-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1518s - loss: 1.4341  \n",
      "Epoch 26/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.3913Epoch 00025: loss improved from 1.43414 to 1.39141, saving model to weights-improvement-25-1.3914-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1213s - loss: 1.3914  \n",
      "Epoch 27/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.3474Epoch 00026: loss improved from 1.39141 to 1.34694, saving model to weights-improvement-26-1.3469-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1169s - loss: 1.3469  \n",
      "Epoch 28/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.3019Epoch 00027: loss improved from 1.34694 to 1.30223, saving model to weights-improvement-27-1.3022-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1219s - loss: 1.3022  \n",
      "Epoch 29/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.2609Epoch 00028: loss improved from 1.30223 to 1.26085, saving model to weights-improvement-28-1.2608-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1200s - loss: 1.2608  \n",
      "Epoch 30/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.2291Epoch 00029: loss improved from 1.26085 to 1.22935, saving model to weights-improvement-29-1.2294-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 977s - loss: 1.2294   \n",
      "Epoch 31/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.1887Epoch 00030: loss improved from 1.22935 to 1.18880, saving model to weights-improvement-30-1.1888-bigger_short.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26598/26598 [==============================] - 899s - loss: 1.1888   \n",
      "Epoch 32/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.1475Epoch 00031: loss improved from 1.18880 to 1.14759, saving model to weights-improvement-31-1.1476-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 928s - loss: 1.1476   \n",
      "Epoch 33/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.1209Epoch 00032: loss improved from 1.14759 to 1.12103, saving model to weights-improvement-32-1.1210-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1031s - loss: 1.1210  \n",
      "Epoch 34/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.0827Epoch 00033: loss improved from 1.12103 to 1.08265, saving model to weights-improvement-33-1.0827-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1005s - loss: 1.0827  \n",
      "Epoch 35/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.0528Epoch 00034: loss improved from 1.08265 to 1.05293, saving model to weights-improvement-34-1.0529-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1037s - loss: 1.0529  \n",
      "Epoch 36/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 1.0267Epoch 00035: loss improved from 1.05293 to 1.02686, saving model to weights-improvement-35-1.0269-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1053s - loss: 1.0269  \n",
      "Epoch 37/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.9917Epoch 00036: loss improved from 1.02686 to 0.99155, saving model to weights-improvement-36-0.9916-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 1050s - loss: 0.9916  \n",
      "Epoch 38/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.9656Epoch 00037: loss improved from 0.99155 to 0.96619, saving model to weights-improvement-37-0.9662-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 819s - loss: 0.9662   \n",
      "Epoch 39/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.9530Epoch 00038: loss improved from 0.96619 to 0.95304, saving model to weights-improvement-38-0.9530-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 815s - loss: 0.9530   \n",
      "Epoch 40/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.9179Epoch 00039: loss improved from 0.95304 to 0.91795, saving model to weights-improvement-39-0.9180-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 819s - loss: 0.9180   \n",
      "Epoch 41/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.8943Epoch 00040: loss improved from 0.91795 to 0.89463, saving model to weights-improvement-40-0.8946-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 812s - loss: 0.8946   \n",
      "Epoch 42/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.8738Epoch 00041: loss improved from 0.89463 to 0.87338, saving model to weights-improvement-41-0.8734-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 808s - loss: 0.8734   \n",
      "Epoch 43/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.8562Epoch 00042: loss improved from 0.87338 to 0.85636, saving model to weights-improvement-42-0.8564-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 806s - loss: 0.8564   \n",
      "Epoch 44/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.8221Epoch 00043: loss improved from 0.85636 to 0.82202, saving model to weights-improvement-43-0.8220-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 805s - loss: 0.8220   \n",
      "Epoch 45/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.8091Epoch 00044: loss improved from 0.82202 to 0.80957, saving model to weights-improvement-44-0.8096-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 799s - loss: 0.8096   \n",
      "Epoch 46/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.7911Epoch 00045: loss improved from 0.80957 to 0.79122, saving model to weights-improvement-45-0.7912-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 796s - loss: 0.7912   \n",
      "Epoch 47/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.7748Epoch 00046: loss improved from 0.79122 to 0.77462, saving model to weights-improvement-46-0.7746-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 804s - loss: 0.7746   \n",
      "Epoch 48/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.7728Epoch 00047: loss improved from 0.77462 to 0.77283, saving model to weights-improvement-47-0.7728-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 802s - loss: 0.7728   \n",
      "Epoch 49/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.7376Epoch 00048: loss improved from 0.77283 to 0.73792, saving model to weights-improvement-48-0.7379-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 805s - loss: 0.7379   \n",
      "Epoch 50/50\n",
      "26560/26598 [============================>.] - ETA: 1s - loss: 0.7208Epoch 00049: loss improved from 0.73792 to 0.72059, saving model to weights-improvement-49-0.7206-bigger_short.hdf5\n",
      "26598/26598 [==============================] - 802s - loss: 0.7206   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b44271c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"short_wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger_short.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  26698\n",
      "Total Vocab:  41\n",
      "Total Patterns:  26598\n",
      "Seed:\n",
      "\" of tears, 'i do wish they would put their heads down! i am so very tired\n",
      "of being all alone here!'\n",
      "\n",
      " \"\n",
      "as she said this the looked down at her hands, and sound the could she tas to get that she was doming to his that she was doming the had seved to fet that she was now, and she soon pasted out to tpeak noe long the remembered to she that whrh and locked in the rome, \n",
      "and she wint blice ln a low of lime to think, tou know, ererything is would shat was now a mouse: tho in she sool of tears, and she thiembered teye this must be the coune she had poment of things, and she, and it sas dorn ald shere was now a moment to thar so get tather she kadel' how sueh tom shought them it she sooedo the simedeed was that she was now, and she soon pade out that it was ooty a mouse, iowever, ohe inceed to the that she had sever hongen kege on the las and peapeng on the way oo esp lnke and the fould she that she was salking hand on a marge rabbit-hald as herself,\n",
      "'and so an inr lake outting\n",
      "tp blice as the simededd tele that it sas to gear the was now, and she soon fare a little little firsing\n",
      "the little g\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"short_wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-49-0.7206-bigger_short.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results are not that good as it has many spelling mistakes and it did not recognize end of sentence with dot. As a beginner, I found it interesting to see automatic generation of text. It's cool !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
